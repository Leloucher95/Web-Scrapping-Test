# ğŸ¯ BrainyQuote Scraper - Plan d'ExÃ©cution

## ğŸ“‹ Cahier des charges (Requirements)

### Description du projet
Web Scraping avec automatisation de navigateur et frontend Nuxt pour extraire des citations de BrainyQuote.com selon un sujet donnÃ© par l'utilisateur.

**Site cible** : https://www.brainyquote.com/
**URL Pattern** : https://www.brainyquote.com/topics/{topic}-quotes
**Exemple** : topic "motivational" â†’ https://www.brainyquote.com/topics/motivational-quotes

### Exigences Backend (Python)
- [x] Outil d'automatisation : **Playwright**
- [ ] Extraction des champs obligatoires :
  1. **Nom de l'auteur**
  2. **Texte de la citation**
  3. **Lien de la citation**
- [ ] Stockage **Supabase Database** (schÃ©ma appropriÃ©)
- [ ] TÃ©lÃ©chargement images â†’ **Supabase Storage**
- [ ] Gestion d'erreurs robuste
- [ ] SystÃ¨me de journalisation (logging)

### Exigences Frontend (TypeScript)
- [x] Framework : **Nuxt.js**
- [x] âœ… Formulaire saisie sujet
- [x] âœ… Bouton lancer scraper
- [ ] ğŸ”„ Indicateur progression (items traitÃ©s)
- [ ] ğŸ”„ Bouton arrÃªter scraper
- [ ] ğŸ”„ Export CSV/JSON des donnÃ©es
- [x] âœ… Interface responsive et conviviale

### Livrables finaux
- [ ] Code backend scraper + intÃ©gration Supabase
- [x] âœ… Frontend Nuxt.js avec UI spÃ©cifiÃ©e
- [ ] Documentation installation/utilisation/dÃ©ploiement

## ğŸ—ï¸ Architecture Technique

### Stack confirmÃ©
- **Backend** : FastAPI + Playwright + Supabase Python Client
- **Frontend** : Nuxt.js 4 + TypeScript + Tailwind + Pinia
- **Database** : Supabase PostgreSQL + Storage
- **Communication** : WebSocket temps rÃ©el + REST API

### Structure fichiers cible
```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Variables env
â”‚   â”‚   â””â”€â”€ logger.py          # Logging setup
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ router.py          # Routes principales
â”‚   â”‚   â””â”€â”€ scraping.py        # Endpoints scraping
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ brainyquote.py     # Scraper BrainyQuote
â”‚   â”‚   â””â”€â”€ browser.py         # Gestion Playwright
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py      # Supabase client
â”‚   â”‚   â””â”€â”€ models.py          # SchÃ©mas DB
â”‚   â””â”€â”€ websocket/
â”‚       â””â”€â”€ manager.py         # WebSocket manager
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

## ğŸ“Š SchÃ©ma Base de DonnÃ©es

### Table `quotes`
```sql
CREATE TABLE quotes (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    author VARCHAR(255) NOT NULL,
    text TEXT NOT NULL,
    link VARCHAR(500) NOT NULL,
    topic VARCHAR(100) NOT NULL,
    image_url VARCHAR(500),        -- URL image BrainyQuote
    image_path VARCHAR(500),       -- Chemin Supabase Storage
    scraped_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    UNIQUE(author, text, topic)    -- Ã‰viter doublons
);
```

### Table `scraping_jobs`
```sql
CREATE TABLE scraping_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    topic VARCHAR(100) NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',  -- pending, running, completed, failed, stopped
    total_items INTEGER DEFAULT 0,
    processed_items INTEGER DEFAULT 0,
    error_message TEXT,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

### Bucket Supabase Storage
```
Bucket: 'quote-images'
Structure: /topic/author-hash.jpg
Exemple: /motivational/einstein-abc123.jpg
```

## ğŸš€ Plan d'ExÃ©cution

### âœ… Phase 0 : Setup Initial (TERMINÃ‰)
- [x] Repository Git
- [x] Frontend Nuxt.js structure conforme
- [x] Documentation troubleshooting
- [x] Structure monorepo
- [x] .env frontend

### ğŸ”„ Phase 1 : Backend Core (EN COURS)
**Objectif** : API fonctionnelle + Scraper basique

#### 1.1 Structure FastAPI
- [ ] `backend/src/main.py` - App FastAPI
- [ ] `backend/requirements.txt` - Dependencies
- [ ] `backend/.env.example` - Variables env
- [ ] Endpoints REST :
  - `POST /api/scrape/start` â†’ lance scraping, retourne job_id
  - `POST /api/scrape/stop/{job_id}` â†’ arrÃªte scraping
  - `GET /api/scrape/status/{job_id}` â†’ statut progression
  - `GET /api/quotes?topic={topic}` â†’ rÃ©cupÃ©rer citations
  - `GET /api/export/{job_id}?format=csv|json` â†’ export donnÃ©es

#### 1.2 Scraper Playwright
- [ ] `backend/src/scraper/brainyquote.py` - Logic scraping
- [ ] Gestion URL : `https://www.brainyquote.com/topics/{topic}-quotes`
- [ ] Extraction champs obligatoires :
  - Nom auteur (selector : Ã  identifier)
  - Texte citation (selector : Ã  identifier)
  - Lien citation (href)
- [ ] Gestion pagination (BrainyQuote â†’ pages multiples)
- [ ] Download images vers Supabase Storage

#### 1.3 IntÃ©gration Supabase
- [ ] `backend/src/database/connection.py` - Client Supabase
- [ ] `backend/src/database/models.py` - SchÃ©mas Pydantic
- [ ] Setup tables (quotes, scraping_jobs)
- [ ] Setup bucket Storage 'quote-images'

#### 1.4 WebSocket Manager
- [ ] `backend/src/websocket/manager.py` - Gestion connexions
- [ ] Ã‰vÃ©nements WebSocket :
  - `scraping_started` â†’ {job_id, topic}
  - `progress_update` â†’ {job_id, processed, total}
  - `quote_found` â†’ {job_id, quote_data}
  - `scraping_completed` â†’ {job_id, total_quotes}
  - `scraping_failed` â†’ {job_id, error}
  - `scraping_stopped` â†’ {job_id}

### ğŸ”„ Phase 2 : Frontend Complet (Ã€ VENIR)
**Objectif** : Interface finale conforme exigences

#### 2.1 FonctionnalitÃ©s manquantes
- [ ] Bouton "ArrÃªter scraper"
- [ ] Indicateur progression temps rÃ©el via WebSocket
- [ ] Affichage citations extraites (temps rÃ©el)
- [ ] Export CSV/JSON avec tÃ©lÃ©chargement
- [ ] Gestion erreurs utilisateur (toasts)

#### 2.2 Plugin WebSocket Frontend
- [ ] `app/plugins/websocket.client.ts` - Connexion auto
- [ ] Mise Ã  jour stores Pinia via Ã©vÃ©nements WS
- [ ] Reconnexion automatique si dÃ©connexion

#### 2.3 Stores Pinia enrichis
- [ ] `app/stores/scraping.ts` - Ã‰tat scraping Ã©tendu
- [ ] `app/stores/quotes.ts` - Liste citations + pagination
- [ ] `app/stores/export.ts` - Gestion export fichiers

### ğŸ§ª Phase 3 : Tests & Polish (Ã€ VENIR)
- [ ] Tests unitaires scraper (pytest)
- [ ] Tests e2e API (FastAPI TestClient)
- [ ] Tests frontend (Vitest)
- [ ] Documentation complÃ¨te
- [ ] Docker setup
- [ ] DÃ©ploiement guide

## ğŸ“ Notes d'ImplÃ©mentation

### URLs BrainyQuote Ã  analyser
```
# Explorer ces URLs pour comprendre la structure
https://www.brainyquote.com/topics/motivational-quotes
https://www.brainyquote.com/topics/love-quotes
https://www.brainyquote.com/topics/success-quotes

# Identifier les sÃ©lecteurs CSS pour :
- Container citations
- Nom auteur
- Texte citation
- Lien citation
- Image citation (si existe)
- Bouton pagination/Next
```

### Gestion erreurs prioritaires
1. **Sujet inexistant** â†’ Page 404 BrainyQuote
2. **Rate limiting** â†’ DÃ©lais entre requÃªtes
3. **Connexion Supabase** â†’ Retry logic
4. **Images indisponibles** â†’ Fallback gracieux
5. **Scraping interrompu** â†’ Nettoyage Ã©tat

### Performance considerations
- **Concurrent scraping** : Max 3 jobs simultanÃ©s
- **Request delay** : 1-2s entre pages BrainyQuote
- **Batch insert** : Grouper insertions DB
- **Image compression** : Optimiser taille Storage

## ğŸ¯ CritÃ¨res de SuccÃ¨s

### Fonctionnel
- [x] âœ… Frontend responsive et intuitif
- [ ] ğŸ”„ Scraping complet d'un sujet donnÃ©
- [ ] ğŸ”„ Stockage correct DB + Storage
- [ ] ğŸ”„ Export CSV/JSON fonctionnel
- [ ] ğŸ”„ ArrÃªt scraping mid-process
- [ ] ğŸ”„ Progression temps rÃ©el

### Technique
- [ ] ğŸ”„ Code modulaire et maintenable
- [ ] ğŸ”„ Gestion erreurs robuste
- [ ] ğŸ”„ Logging complet
- [ ] ğŸ”„ Documentation installation/usage
- [ ] ğŸ”„ Performance acceptable (< 5s/page)

---

## ğŸš§ Statut Actuel : Phase 1.1 Backend Structure

**Prochaine action** : CrÃ©er structure FastAPI + requirements.txt + premier endpoint de test.